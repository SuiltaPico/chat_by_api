### 对于当前大型语言模型模型阅读超大文本的研究

超大文本指的是解析成 Token 后超过语言模型 Token 上限的文本。

- [pdfGPT](https://github.com/bhaskatripathi/pdfGPT) 是一个使用 GPT 与 PDF 文件的内容进行聊天的项目。做法是将 PDF 拆分成一个个 chunk。使用 Universal Sentence Encoder 转换成一个个向量，然后使用语义搜索算法搜索 top n 的 chunks。将所得的 top n chunks 嵌入大型语言模型中。

信息就像一个音频文件，要储存越细节（高频）的信息需要越多存储空间。仅将原文档丢给 GPT 需要消耗太多 token，因为在很多时候，我们并不需要这么多细节。例如：
```
用户：第三章的内容讲了什么？
系统嵌入了一大堆文本，是第三章零零碎碎的细节。
语言模型耗费了一大堆资源计算，然后进行总结。
```

应该渐进式地拆分文档，然后把投入细节的选择权交给语言模型：
```
文档储存分成三级。源文档、一次总结文档，二次总结文档。多次总结文档来源于10个上一层的文档的总结。
用户：第三章的内容讲了什么？
系统匹配文档标题，选择一些文档，然后让语言模型选择文档。
语言模型选择文档。
系统将选择的文档嵌入语言模型的对话。
语言模型仅消耗了较少的资源，准确回答用户的问题。
```

### 解决办法

1. 嵌入向量 + 关联度
- 步骤：
1. 用户拆分文本为 chunk
2. 利用嵌入模型对每个 chunk 生成向量
3. 先按照高斯分布设置 chunk 与相邻 chunk 的关联度
4. 用户手动设置文档关联度
5. 将向量和文本一同存入数据库。
6. 读取时，使用 hnsw 算法搜索邻近向量，提取关联文本，按照用户设置的权重和 token 上限综合计算，选择最优的 top n 文档，内嵌入大型语言模型的 prompt 文本。

- 优点：
  1. 可以更好地处理复杂的章节结构和上下文信息，提高搜索结果的准确性。
  2. 可以适应各种类型的文本等。
  3. 在计算资源较少的情况下也能快速响应。
- 缺点：
  1. 需要用户有一定的专业知识和经验。
  2. 可能会增加用户的工作负担，特别是在处理大量文本时。
  - 在拆分、标注关联度时能提供更好的工具（文本搜索）帮助用户进行 chunk 构筑。
- 指标预估：
  - 计算资源需求：低
  - 计算速度：快
  - 用户交互复杂度：高
  - 准确度：极高~中等（取决于用户个人水平）

2. 嵌入向量 + 信息检索：
- 步骤：
  1. 用户拆分文本为 chunk
  2. 利用嵌入模型对每个 chunk 生成向量
  3. 将向量和文本一同存入数据库。
  4. 读取时，使用 hnsw 算法搜索邻近向量，使用信息检索技术（例如 BM25 算法、倒排索引）搜索相关文本，按照用户设置的权重和 token 上限综合计算，提取出 top n 文档，内嵌入大型语言模型的 prompt 文本。

- 优点：
  1. 信息检索技术可以有效地处理大规模文本数据，提高搜索效率。
  2. 通过嵌入向量，可以捕捉到文本的语义信息，提高搜索结果的相关性。
- 缺点：
  1. 信息检索技术可能无法完全理解文本的深层含义，可能会出现一些误检。
  2. 对于长文本的处理可能不够理想，可能需要进一步的优化。

- 指标预估：
  - 计算资源需求：低
  - 计算速度：快
  - 用户交互复杂度：低
  - 准确度：中等

3. 嵌入向量 + 深度学习模型：用户拆分文本为 chunk，利用嵌入模型生成向量，将向量和文本一同存入数据库。读取时，使用深度学习模型（例如 BERT、Transformer 等）进行文本匹配，提取出 top n 文档，内嵌入大型语言模型的 prompt 文本。

- 优点：
  1. 深度学习模型可以理解文本的深层含义，提高搜索结果的准确性。
  2. 可以处理各种类型的文本，包括长文本、短文本等。
- 缺点：
  1. 深度学习模型的训练和调优需要极其大量的计算资源和时间。
- 指标预估：
  - 计算资源需求：极高~高（随着准确度的增高而增高）
  - 计算速度：慢
  - 用户交互复杂度：低
  - 准确度：极高~中等（取决于深度学习模型准确程度）